{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic MuZero Training for 2048 (JAX)\n",
    "\n",
    "This notebook trains a Stochastic MuZero agent to play 2048 using JAX.\n",
    "\n",
    "**Supported Hardware:**\n",
    "- NVIDIA Tesla P100/T4 (Kaggle GPU)\n",
    "- TPU v3-8 (Kaggle TPU)\n",
    "- CPU (slower, but works anywhere)\n",
    "\n",
    "**Paper:** \"Planning in Stochastic Environments with a Learned Model\" (ICLR 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##>: Clone repository and checkout the correct branch.\n",
    "# ##!: Replace with your actual repository URL before running.\n",
    "REPO_URL = 'https://github.com/YOUR_USERNAME/simulate_2048.git'  # CHANGE THIS\n",
    "BRANCH = 'muzero'  # The branch containing the training code\n",
    "\n",
    "!git clone {REPO_URL}\n",
    "%cd simulate_2048\n",
    "!git checkout {BRANCH}\n",
    "!git log --oneline -3  # Verify we're on the correct branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##>: Install JAX dependencies.\n",
    "# ##&: Kaggle GPU has CUDA, so install jax[cuda12].\n",
    "!pip install -q jax[cuda12] flax optax mctx chex orbax-checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ##>: Add project root to path.\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f'Project root: {project_root}')\n",
    "print(f'Python version: {sys.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hardware Detection\n",
    "\n",
    "Detects GPU/TPU availability and configures JAX appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "print(f'JAX version: {jax.__version__}')\n",
    "print(f'Available devices: {jax.devices()}')\n",
    "print(f'Default backend: {jax.default_backend()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##>: Detect available hardware.\n",
    "print('=' * 60)\n",
    "print('HARDWARE CONFIGURATION')\n",
    "print('=' * 60)\n",
    "\n",
    "devices = jax.devices()\n",
    "backend = jax.default_backend()\n",
    "\n",
    "if backend == 'gpu':\n",
    "    print(f'GPU detected: {len(devices)} device(s)')\n",
    "    for i, dev in enumerate(devices):\n",
    "        print(f'  - Device {i}: {dev}')\n",
    "    ACCELERATOR = 'GPU'\n",
    "elif backend == 'tpu':\n",
    "    print(f'TPU detected: {len(devices)} core(s)')\n",
    "    ACCELERATOR = 'TPU'\n",
    "else:\n",
    "    print('No GPU/TPU detected, using CPU')\n",
    "    ACCELERATOR = 'CPU'\n",
    "\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Configure hyperparameters based on detected hardware and available runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##>: Training configuration.\n",
    "# ##!: Adjust these based on Kaggle session limits (12h for GPU, 9h for TPU).\n",
    "\n",
    "# ##>: Choose training mode.\n",
    "TRAINING_MODE = 'small'  # Options: 'tiny' (testing), 'small' (balanced), 'full' (paper)\n",
    "\n",
    "# ##>: Training duration.\n",
    "# ##&: 10k steps is reasonable for a single Kaggle session (~2-4 hours on P100).\n",
    "NUM_TRAINING_STEPS = 10_000  # Adjust based on available time\n",
    "\n",
    "# ##>: Checkpointing.\n",
    "CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "\n",
    "# ##>: Random seed for reproducibility.\n",
    "SEED = 42\n",
    "\n",
    "print(f'Training mode: {TRAINING_MODE}')\n",
    "print(f'Training steps: {NUM_TRAINING_STEPS:,}')\n",
    "print(f'Checkpoint directory: {CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforce.training.config import TrainConfig, default_config, small_config, tiny_config\n",
    "\n",
    "\n",
    "def create_kaggle_config(mode: str, accelerator: str, num_steps: int) -> TrainConfig:\n",
    "    \"\"\"\n",
    "    Create configuration optimized for Kaggle hardware.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : str\n",
    "        Training mode: 'tiny', 'small', or 'full'.\n",
    "    accelerator : str\n",
    "        Detected hardware: 'GPU', 'TPU', or 'CPU'.\n",
    "    num_steps : int\n",
    "        Total training steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TrainConfig\n",
    "        Optimized configuration.\n",
    "    \"\"\"\n",
    "    if mode == 'tiny':\n",
    "        base = tiny_config()\n",
    "    elif mode == 'full':\n",
    "        base = default_config()\n",
    "    else:  # small\n",
    "        base = small_config()\n",
    "\n",
    "    # ##>: Adjust for hardware.\n",
    "    if accelerator == 'GPU':\n",
    "        batch_size = min(base.batch_size, 512)\n",
    "    elif accelerator == 'TPU':\n",
    "        batch_size = min(base.batch_size, 1024)\n",
    "    else:\n",
    "        batch_size = min(base.batch_size, 128)\n",
    "\n",
    "    # ##>: Create new config with adjusted parameters.\n",
    "    return TrainConfig(\n",
    "        observation_shape=base.observation_shape,\n",
    "        action_size=base.action_size,\n",
    "        codebook_size=base.codebook_size,\n",
    "        hidden_size=base.hidden_size,\n",
    "        num_residual_blocks=base.num_residual_blocks,\n",
    "        num_simulations=base.num_simulations,\n",
    "        discount=base.discount,\n",
    "        dirichlet_alpha=base.dirichlet_alpha,\n",
    "        dirichlet_fraction=base.dirichlet_fraction,\n",
    "        pb_c_init=base.pb_c_init,\n",
    "        pb_c_base=base.pb_c_base,\n",
    "        replay_buffer_size=base.replay_buffer_size,\n",
    "        min_buffer_size=base.min_buffer_size,\n",
    "        max_trajectory_length=base.max_trajectory_length,\n",
    "        batch_size=batch_size,\n",
    "        num_unroll_steps=base.num_unroll_steps,\n",
    "        td_steps=base.td_steps,\n",
    "        td_lambda=base.td_lambda,\n",
    "        learning_rate=base.learning_rate,\n",
    "        weight_decay=base.weight_decay,\n",
    "        max_grad_norm=base.max_grad_norm,\n",
    "        warmup_steps=base.warmup_steps,\n",
    "        training_steps=num_steps,\n",
    "        checkpoint_interval=base.checkpoint_interval,\n",
    "        log_interval=base.log_interval,\n",
    "        eval_interval=base.eval_interval,\n",
    "        eval_games=base.eval_games,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "\n",
    "config = create_kaggle_config(TRAINING_MODE, ACCELERATOR, NUM_TRAINING_STEPS)\n",
    "\n",
    "print('\\nConfiguration:')\n",
    "print(f'  Hidden size: {config.hidden_size}')\n",
    "print(f'  Residual blocks: {config.num_residual_blocks}')\n",
    "print(f'  Batch size: {config.batch_size}')\n",
    "print(f'  MCTS simulations: {config.num_simulations}')\n",
    "print(f'  Replay buffer size: {config.replay_buffer_size:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Trainer\n",
    "\n",
    "Create the JAX-based trainer with the configured settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforce.training.trainer import Trainer\n",
    "\n",
    "# ##>: Create checkpoint directory.\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ##>: Initialize trainer.\n",
    "trainer = Trainer(\n",
    "    config=config,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    ")\n",
    "\n",
    "# ##>: Initialize training state.\n",
    "trainer.initialize(seed=SEED)\n",
    "\n",
    "print('Trainer initialized successfully.')\n",
    "print(f'Checkpoints will be saved to: {CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fill Replay Buffer\n",
    "\n",
    "Generate initial games to populate the replay buffer before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Generating initial games for replay buffer...')\nprint(f'Target: {config.min_buffer_size} trajectories')\nprint()\n\ntrainer.fill_buffer(show_progress=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "Run the main training loop with progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting training...')\n",
    "print(f'Hardware: {ACCELERATOR}')\n",
    "print(f'Steps: {NUM_TRAINING_STEPS:,}')\n",
    "print()\n",
    "\n",
    "results = trainer.train(\n",
    "    num_steps=NUM_TRAINING_STEPS,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "print('\\n' + '=' * 50)\n",
    "print('Training Complete')\n",
    "print('=' * 50)\n",
    "print(f'Total steps: {results[\"total_steps\"]}')\n",
    "print(f'Total time: {results[\"total_time_seconds\"]:.1f}s')\n",
    "print(f'Steps/second: {results[\"steps_per_second\"]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('FINAL EVALUATION (20 games)')\n",
    "print('=' * 60)\n",
    "\n",
    "final_eval = trainer.evaluate(num_games=20, show_progress=True)\n",
    "\n",
    "print(f'\\nMean Reward: {final_eval[\"mean_reward\"]:.1f}')\n",
    "print(f'Max Reward: {final_eval[\"max_reward\"]:.1f}')\n",
    "print(f'Mean Max Tile: {final_eval[\"mean_max_tile\"]:.1f}')\n",
    "print(f'Best Tile Achieved: {final_eval[\"max_tile\"]}')\n",
    "print(f'Mean Game Length: {final_eval[\"mean_length\"]:.1f} moves')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##>: Save final checkpoint.\n",
    "trainer.save_checkpoint()\n",
    "print(f'Final checkpoint saved.')\n",
    "\n",
    "# ##>: List all checkpoints.\n",
    "import glob\n",
    "\n",
    "checkpoints = sorted(glob.glob(f'{CHECKPOINT_DIR}/*'))\n",
    "print(f'\\nAll checkpoints ({len(checkpoints)}):')  \n",
    "for cp in checkpoints:\n",
    "    print(f'  {cp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Checkpoints\n",
    "\n",
    "Package checkpoints for download from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# ##>: Create zip archive of checkpoints.\n",
    "archive_path = '/kaggle/working/checkpoints_archive'\n",
    "shutil.make_archive(archive_path, 'zip', CHECKPOINT_DIR)\n",
    "print(f'Checkpoints archived to: {archive_path}.zip')\n",
    "\n",
    "# ##>: Show file size.\n",
    "archive_size = os.path.getsize(f'{archive_path}.zip') / (1024 * 1024)\n",
    "print(f'Archive size: {archive_size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "**Hardware-specific optimizations:**\n",
    "\n",
    "- **Tesla P100/T4**: Uses CUDA backend with optimized batch sizes.\n",
    "- **TPU**: Uses TPU backend with larger batch sizes for better utilization.\n",
    "- **CPU**: Uses smaller batch sizes for memory efficiency.\n",
    "\n",
    "**Training tips:**\n",
    "\n",
    "1. Start with `TRAINING_MODE = 'tiny'` to verify everything works.\n",
    "2. Use `TRAINING_MODE = 'small'` for a balance of quality and speed.\n",
    "3. The paper uses 20M steps with `TRAINING_MODE = 'full'` - this requires multiple sessions.\n",
    "\n",
    "**JAX advantages:**\n",
    "\n",
    "- JIT compilation for faster execution after warmup\n",
    "- Automatic vectorization with vmap\n",
    "- Native TPU support\n",
    "- Functional programming paradigm for better reproducibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}