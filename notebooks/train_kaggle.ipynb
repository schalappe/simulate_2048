{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Stochastic MuZero Training for 2048\n\nThis notebook trains a Stochastic MuZero agent to play 2048.\n\n**Supported Hardware:**\n- NVIDIA Tesla P100 (16GB VRAM)\n- CPU (slower, but works anywhere)\n\n**Paper:** \"Planning in Stochastic Environments with a Learned Model\" (ICLR 2022)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##>: Clone repository and checkout the correct branch.\n",
    "# ##!: Replace with your actual repository URL before running.\n",
    "REPO_URL = 'https://github.com/YOUR_USERNAME/simulate_2048.git'  # CHANGE THIS\n",
    "BRANCH = 'muzero'  # The branch containing the training code\n",
    "\n",
    "!git clone {REPO_URL}\n",
    "%cd simulate_2048\n",
    "!git checkout {BRANCH}\n",
    "!git log --oneline -3  # Verify we're on the correct branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##>: Install dependencies.\n",
    "# ##&: Kaggle has most dependencies pre-installed, we only need gymnasium.\n",
    "!pip install -q gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ##>: Add project root to path.\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f'Project root: {project_root}')\n",
    "print(f'Python version: {sys.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Hardware Detection\n\nDetects GPU availability and configures TensorFlow appropriately."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'Keras version: {tf.keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# ##>: Detect available hardware.\n",
    "print('=' * 60)\n",
    "print('HARDWARE CONFIGURATION')\n",
    "print('=' * 60)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f'GPU detected: {len(gpus)} device(s)')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f'  - {details.get(\"device_name\", \"Unknown GPU\")}')\n",
    "    ACCELERATOR = 'GPU'\n",
    "else:\n",
    "    print('No GPU detected, using CPU')\n",
    "    ACCELERATOR = 'CPU'\n",
    "\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Configure hyperparameters based on detected hardware and available runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##>: Training configuration.\n",
    "# ##!: Adjust these based on Kaggle session limits (12h for GPU, 9h for TPU).\n",
    "\n",
    "# ##>: Choose training mode.\n",
    "TRAINING_MODE = 'medium'  # Options: 'small' (testing), 'medium' (balanced), 'full' (paper)\n",
    "\n",
    "# ##>: Training duration.\n",
    "# ##&: 50k steps is reasonable for a single Kaggle session (~4-6 hours on P100).\n",
    "# ##&: For longer training, use checkpoints to resume across sessions.\n",
    "NUM_TRAINING_STEPS = 50_000  # Adjust based on available time\n",
    "\n",
    "# ##>: Checkpointing.\n",
    "CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "CHECKPOINT_INTERVAL = 5_000\n",
    "\n",
    "# ##>: Logging and evaluation.\n",
    "LOG_INTERVAL = 100\n",
    "EVAL_INTERVAL = 5_000\n",
    "EVAL_GAMES = 10\n",
    "\n",
    "# ##>: Game generation frequency.\n",
    "# ##&: Higher values generate more diverse training data but slow training.\n",
    "GAMES_PER_STEP = 2  # Generate 2 games per training step (every 10 steps)\n",
    "\n",
    "print(f'Training mode: {TRAINING_MODE}')\n",
    "print(f'Training steps: {NUM_TRAINING_STEPS:,}')\n",
    "print(f'Checkpoint interval: {CHECKPOINT_INTERVAL:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from reinforce.training.config import StochasticMuZeroConfig, default_2048_config, small_2048_config\n\n\ndef create_kaggle_config(mode: str, accelerator: str, num_steps: int) -> StochasticMuZeroConfig:\n    \"\"\"\n    Create configuration optimized for Kaggle hardware.\n\n    Parameters\n    ----------\n    mode : str\n        Training mode: 'small', 'medium', or 'full'.\n    accelerator : str\n        Detected hardware: 'GPU' or 'CPU'.\n    num_steps : int\n        Total training steps (used to adjust temperature schedule).\n\n    Returns\n    -------\n    StochasticMuZeroConfig\n        Optimized configuration.\n    \"\"\"\n    if mode == 'small':\n        config = small_2048_config()\n    elif mode == 'full':\n        config = default_2048_config()\n    else:  # medium\n        config = StochasticMuZeroConfig(\n            hidden_size=192,\n            num_residual_blocks=7,\n            num_simulations=75,\n            replay_buffer_size=50_000,\n            batch_size=512,\n            training_steps=num_steps,\n        )\n\n    # ##>: Adjust batch sizes based on hardware.\n    if accelerator == 'GPU':\n        # ##&: P100 has 16GB; keep batch size reasonable.\n        config.batch_size = min(config.batch_size, 512)\n    else:\n        # ##>: CPU uses smaller batches.\n        config.batch_size = min(config.batch_size, 128)\n\n    # ##>: Adjust temperature schedule for shorter training.\n    # ##&: Default schedule expects 300k+ steps; scale down proportionally.\n    if num_steps < 300_000:\n        scale = num_steps / 300_000\n        config.temperature_schedule = [\n            (0, 1.0),\n            (int(100_000 * scale), 0.5),\n            (int(200_000 * scale), 0.1),\n            (int(250_000 * scale), 0.0),\n        ]\n\n    return config\n\n\nconfig = create_kaggle_config(TRAINING_MODE, ACCELERATOR, NUM_TRAINING_STEPS)\n\nprint('\\nConfiguration:')\nprint(f'  Hidden size: {config.hidden_size}')\nprint(f'  Residual blocks: {config.num_residual_blocks}')\nprint(f'  Batch size: {config.batch_size}')\nprint(f'  MCTS simulations: {config.num_simulations}')\nprint(f'  Replay buffer size: {config.replay_buffer_size:,}')\nprint(f'  Temperature schedule: {config.temperature_schedule}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Trainer\n",
    "\n",
    "Create the trainer with the configured settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforce.training.trainer import StochasticMuZeroTrainer\n",
    "\n",
    "# ##>: Create checkpoint directory.\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ##>: Initialize trainer.\n",
    "trainer = StochasticMuZeroTrainer(\n",
    "    config=config,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    ")\n",
    "\n",
    "print('Trainer initialized successfully.')\n",
    "print(f'Checkpoints will be saved to: {CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with Monitoring\n",
    "\n",
    "Run training with progress tracking and periodic evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "class TrainingMonitor:\n",
    "    \"\"\"\n",
    "    Monitor training progress with live plots.\n",
    "\n",
    "    Tracks losses, rewards, and max tiles achieved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, update_interval: int = 500):\n",
    "        self.update_interval = update_interval\n",
    "        self.steps: list[int] = []\n",
    "        self.losses: list[float] = []\n",
    "        self.rewards: list[float] = []\n",
    "        self.max_tiles: list[int] = []\n",
    "        self.eval_steps: list[int] = []\n",
    "        self.eval_rewards: list[float] = []\n",
    "        self.eval_tiles: list[int] = []\n",
    "\n",
    "    def update(self, step: int, metrics: dict) -> None:\n",
    "        \"\"\"Record metrics and update plots periodically.\"\"\"\n",
    "        self.steps.append(step)\n",
    "        self.losses.append(metrics['losses']['total'])\n",
    "        self.rewards.append(metrics['avg_reward'])\n",
    "\n",
    "        if step > 0 and step % self.update_interval == 0:\n",
    "            self._plot()\n",
    "\n",
    "    def add_eval(self, step: int, reward: float, max_tile: int) -> None:\n",
    "        \"\"\"Record evaluation results.\"\"\"\n",
    "        self.eval_steps.append(step)\n",
    "        self.eval_rewards.append(reward)\n",
    "        self.eval_tiles.append(max_tile)\n",
    "\n",
    "    def _plot(self) -> None:\n",
    "        \"\"\"Generate training progress plots.\"\"\"\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "        # ##>: Loss plot.\n",
    "        axes[0].plot(self.steps, self.losses, 'b-', alpha=0.7, linewidth=0.5)\n",
    "        if len(self.losses) > 100:\n",
    "            # ##>: Moving average for smoothing.\n",
    "            window = min(100, len(self.losses) // 10)\n",
    "            smoothed = [\n",
    "                sum(self.losses[max(0, i - window) : i + 1]) / min(i + 1, window) for i in range(len(self.losses))\n",
    "            ]\n",
    "            axes[0].plot(self.steps, smoothed, 'r-', linewidth=2, label='Smoothed')\n",
    "        axes[0].set_xlabel('Step')\n",
    "        axes[0].set_ylabel('Total Loss')\n",
    "        axes[0].set_title('Training Loss')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        # ##>: Reward plot.\n",
    "        axes[1].plot(self.steps, self.rewards, 'g-', alpha=0.7, linewidth=0.5)\n",
    "        if self.eval_steps:\n",
    "            axes[1].scatter(self.eval_steps, self.eval_rewards, c='red', s=50, zorder=5, label='Eval')\n",
    "        axes[1].set_xlabel('Step')\n",
    "        axes[1].set_ylabel('Average Reward')\n",
    "        axes[1].set_title('Episode Reward')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        if self.eval_steps:\n",
    "            axes[1].legend()\n",
    "\n",
    "        # ##>: Max tile plot (evaluation only).\n",
    "        if self.eval_steps:\n",
    "            axes[2].bar(range(len(self.eval_tiles)), self.eval_tiles, color='purple', alpha=0.7)\n",
    "            axes[2].set_xticks(range(len(self.eval_tiles)))\n",
    "            axes[2].set_xticklabels([f'{s // 1000}k' for s in self.eval_steps], rotation=45)\n",
    "            axes[2].axhline(y=2048, color='r', linestyle='--', label='2048')\n",
    "            axes[2].axhline(y=4096, color='g', linestyle='--', label='4096')\n",
    "        axes[2].set_xlabel('Evaluation')\n",
    "        axes[2].set_ylabel('Max Tile')\n",
    "        axes[2].set_title('Best Tile Achieved')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        if self.eval_steps:\n",
    "            axes[2].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # ##>: Print summary.\n",
    "        print(f'\\nStep {self.steps[-1]:,} / {NUM_TRAINING_STEPS:,}')\n",
    "        print(f'  Loss: {self.losses[-1]:.4f}')\n",
    "        print(f'  Avg Reward: {self.rewards[-1]:.1f}')\n",
    "        if self.eval_tiles:\n",
    "            print(f'  Best Tile: {max(self.eval_tiles)}')\n",
    "\n",
    "\n",
    "monitor = TrainingMonitor(update_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_callback(step: int, metrics: dict) -> None:\n",
    "    \"\"\"\n",
    "    Callback for training loop.\n",
    "\n",
    "    Updates monitor and runs periodic evaluation.\n",
    "    \"\"\"\n",
    "    monitor.update(step, metrics)\n",
    "\n",
    "    # ##>: Periodic evaluation.\n",
    "    if step > 0 and step % EVAL_INTERVAL == 0:\n",
    "        eval_results = trainer.evaluate(num_games=EVAL_GAMES)\n",
    "        monitor.add_eval(step, eval_results['mean_reward'], eval_results['max_tile'])\n",
    "        print(f'\\n[Eval @ {step:,}] Reward: {eval_results[\"mean_reward\"]:.1f}, Max Tile: {eval_results[\"max_tile\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##>: Run training.\n",
    "print('Starting training...')\n",
    "print(f'Hardware: {ACCELERATOR}')\n",
    "print(f'Steps: {NUM_TRAINING_STEPS:,}')\n",
    "print()\n",
    "\n",
    "history = trainer.train(\n",
    "    num_steps=NUM_TRAINING_STEPS,\n",
    "    log_interval=LOG_INTERVAL,\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "    games_per_step=GAMES_PER_STEP,\n",
    "    callback=training_callback,\n",
    ")\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('FINAL EVALUATION (20 games)')\n",
    "print('=' * 60)\n",
    "\n",
    "final_eval = trainer.evaluate(num_games=20)\n",
    "\n",
    "print(f'Mean Reward: {final_eval[\"mean_reward\"]:.1f}')\n",
    "print(f'Max Reward: {final_eval[\"max_reward\"]:.1f}')\n",
    "print(f'Mean Max Tile: {final_eval[\"mean_max_tile\"]:.1f}')\n",
    "print(f'Best Tile Achieved: {final_eval[\"max_tile\"]}')\n",
    "print(f'Mean Game Length: {final_eval[\"mean_length\"]:.1f} moves')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##>: Save final checkpoint.\n",
    "final_checkpoint = f'{CHECKPOINT_DIR}/final'\n",
    "trainer.learner.save_checkpoint(final_checkpoint)\n",
    "print(f'Final model saved to: {final_checkpoint}')\n",
    "\n",
    "# ##>: List all checkpoints.\n",
    "import glob\n",
    "\n",
    "checkpoints = sorted(glob.glob(f'{CHECKPOINT_DIR}/*'))\n",
    "print(f'\\nAll checkpoints ({len(checkpoints)}):')\n",
    "for cp in checkpoints:\n",
    "    print(f'  {cp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ##>: Total loss.\n",
    "axes[0, 0].plot(history['total_loss'], 'b-', alpha=0.5, linewidth=0.5)\n",
    "window = max(1, len(history['total_loss']) // 100)\n",
    "smoothed = [\n",
    "    sum(history['total_loss'][max(0, i - window) : i + 1]) / min(i + 1, window)\n",
    "    for i in range(len(history['total_loss']))\n",
    "]\n",
    "axes[0, 0].plot(smoothed, 'r-', linewidth=2)\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ##>: Component losses.\n",
    "axes[0, 1].plot(history['policy_loss'], label='Policy', alpha=0.7)\n",
    "axes[0, 1].plot(history['value_loss'], label='Value', alpha=0.7)\n",
    "axes[0, 1].plot(history['reward_loss'], label='Reward', alpha=0.7)\n",
    "axes[0, 1].plot(history['chance_loss'], label='Chance', alpha=0.7)\n",
    "axes[0, 1].set_title('Component Losses')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# ##>: Episode reward.\n",
    "if history['episode_reward']:\n",
    "    axes[1, 0].plot(history['episode_reward'], 'g-', alpha=0.5, linewidth=0.5)\n",
    "    window = max(1, len(history['episode_reward']) // 50)\n",
    "    smoothed = [\n",
    "        sum(history['episode_reward'][max(0, i - window) : i + 1]) / min(i + 1, window)\n",
    "        for i in range(len(history['episode_reward']))\n",
    "    ]\n",
    "    axes[1, 0].plot(smoothed, 'darkgreen', linewidth=2)\n",
    "axes[1, 0].set_title('Episode Reward')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Reward')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ##>: Max tile distribution.\n",
    "if history['max_tile']:\n",
    "    tile_counts = {}\n",
    "    for tile in history['max_tile']:\n",
    "        tile_counts[tile] = tile_counts.get(tile, 0) + 1\n",
    "    tiles = sorted(tile_counts.keys())\n",
    "    counts = [tile_counts[t] for t in tiles]\n",
    "    axes[1, 1].bar([str(t) for t in tiles], counts, color='purple', alpha=0.7)\n",
    "    axes[1, 1].set_title('Max Tile Distribution')\n",
    "    axes[1, 1].set_xlabel('Tile Value')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHECKPOINT_DIR}/training_summary.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nSummary plot saved to: {CHECKPOINT_DIR}/training_summary.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Checkpoints\n",
    "\n",
    "Package checkpoints for download from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# ##>: Create zip archive of checkpoints.\n",
    "archive_path = '/kaggle/working/checkpoints_archive'\n",
    "shutil.make_archive(archive_path, 'zip', CHECKPOINT_DIR)\n",
    "print(f'Checkpoints archived to: {archive_path}.zip')\n",
    "\n",
    "# ##>: Show file size.\n",
    "archive_size = os.path.getsize(f'{archive_path}.zip') / (1024 * 1024)\n",
    "print(f'Archive size: {archive_size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Notes\n\n**Hardware-specific optimizations:**\n\n- **Tesla P100**: Uses default strategy with memory growth enabled. 16GB VRAM supports batch sizes up to 512.\n- **CPU**: Uses smaller batch sizes for memory efficiency.\n\n**Training tips:**\n\n1. Start with `TRAINING_MODE = 'small'` to verify everything works.\n2. Use `TRAINING_MODE = 'medium'` for a balance of quality and speed.\n3. The paper uses 20M steps with `TRAINING_MODE = 'full'` - this requires multiple sessions.\n\n**Resuming training:**\n\n```python\ntrainer.load_checkpoint('/kaggle/working/checkpoints/step_10000')\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}